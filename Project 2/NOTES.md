## NOTES
 Note some interesting discovers about your work in an organised manner

Initializattion of priors and posteriors: Any better way to do it?
Probs: Write the function properly
KL_divergence and Loss function: Something is not working properly
-> Why LogNormal?
-> What does kl_divergence (torch function) do?
-> Should we rescale the kl_loss to the number of weights?: https://gluon.mxnet.io/chapter18_variational-methods-and-uncertainty/bayes-by-backprop.html

